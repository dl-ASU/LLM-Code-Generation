{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1VpOAmF_mHR",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers -U -q accelerate -U protobuf scipy utils datasets pandas matplotlib seaborn gdown sentencepiece nltk evaluate rouge_score\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U einops\n",
        "!pip install -q -U safetensors\n",
        "!pip install -q -U torch\n",
        "!pip install -q -U xformers\n",
        "!pip install -q -U datasets\n",
        "!pip install tqdm\n",
        "!pip install -U langchain openai chromadb langchainhub bs4\n",
        "\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from utils import *\n",
        "\n",
        "# from google.colab import drive\n",
        "import pandas as pd\n",
        "import datasets\n",
        "\n",
        "import pandas as pd\n",
        "# Install the transformers library if you haven't already\n",
        "!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BartTokenizer, BartModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_from_drive(file_url, output_path):\n",
        "    # Extract file ID from URL\n",
        "    file_id = file_url.split(\"/d/\")[1].split(\"/view\")[0]\n",
        "\n",
        "    # Construct download link\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Download file\n",
        "    gdown.download(download_url, output_path, quiet=False)\n",
        "\n",
        "# Load the data from my drive\n",
        "import gdown\n",
        "load_data_from_drive(\"https://drive.google.com/file/d/10VUTIoxedFTR5WJH3gV6f7s6o2ZGoXbm/view?usp=drive_link\", \"Subset0.csv\")\n",
        "df = pd.read_csv('Subset0.csv')\n",
        "\n",
        "print(df.solutions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = OpenAI( api_key = \"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\")\n",
        "\n",
        "def make_prompt(prompt, model=\"gpt-4-1106-preview\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "# import dotenv\n",
        "\n",
        "# dotenv.load_dotenv()\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "\n",
        "loader = CSVLoader(file_path='Subset0.csv', source_column=\"Solutions\")\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "print(data)\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "print(len(all_splits))\n",
        "\n",
        "len(all_splits[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do not run, not our case\n",
        "\n",
        "PYTHON_CODE = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "\n",
        "# Call the function\n",
        "hello_world()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# in our case: each sample has its PYTHON_CODE , I thinks code for each sample need no splitting\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "python_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "\n",
        "# Put the code inside this function\n",
        "retrieved_docs = retriever.get_relevant_documents(\n",
        "    \"Put the code inside this function ...........\"\n",
        ")\n",
        "\n",
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to generate test case for the givin code.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "rag_prompt_custom = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt_custom\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"Write your code here ......................\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Open Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JseE-h0VqIH",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def generate_one_completion(prompt: str):\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
        "\n",
        "    # Generate\n",
        "    generate_ids = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n",
        "    completion = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    completion = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\n",
        "\n",
        "    return completion\n",
        "\n",
        "# initialize the model\n",
        "model_path = \"Phind/Phind-CodeLlama-34B-v2\"\n",
        "model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsVRYZk310BD"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True).cuda()\n",
        "\n",
        "# messages = [\n",
        "#     { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load BART model and tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartModel.from_pretrained('facebook/bart-large-cnn').to(device)\n",
        "\n",
        "# Function to encode text and return embeddings\n",
        "def encode_text(text):\n",
        "    # Tokenize and encode the text\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
        "    inputs = inputs.to('cuda')\n",
        "    # Forward pass through the model to get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the embeddings (last layer hidden states)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    return embeddings.cpu().numpy()\n",
        "\n",
        "\n",
        "embeddings_list = []\n",
        "\n",
        "# Encode each description in the dataset and append to the list\n",
        "for solution in df['solutions']:\n",
        "    embeddings = encode_text(solution)\n",
        "    embeddings_list.append(embeddings)\n",
        "\n",
        "\n",
        "np.save('Embeddings.npy', embeddings_list)\n",
        "\n",
        "# prompt: turn embeddings_list into np array\n",
        "\n",
        "embeddings_list = np.load('Embeddings.npy')\n",
        "embeddings_list = embeddings_list.reshape(100, 1024)\n",
        "embeddings_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cosine_similarity(user_input_embedding, dataset_embeddings):\n",
        "\n",
        "    user_input_embedding = user_input_embedding.reshape(1, -1)\n",
        "    similarities = cosine_similarity(user_input_embedding, dataset_embeddings)\n",
        "    return similarities[0]\n",
        "\n",
        "# User input\n",
        "user_input = \"\"\"for _ in range(input()):\\\\n    try:\\\\n        eval(raw_input())\\\\n        print \\'YES\\'\\\\n    except TypeError:\\\\n\n",
        "    print \\'YES\\'\\\\n    except:\\\\n        print \\'NO\\'\",\\n       \\'for _ in range(input()):\\\\n    ins = raw_input().strip()\\\\n    stck = []\\\\n    res = \"YES\"\\\\n\n",
        "     for x in ins:\\\\n        if x == \"(\":\\\\n            stck.append(x)\\\\n        else:\\\\n            if len(stck)>0:\\\\n\n",
        "      stck.pop()\\\\n            else:\\\\n                res = \"NO\"\\\\n                break\\\\n    if len(stck) > 0: res = \"NO\" \\\\n    print res\\',\\n\n",
        "        \"for _ in range(input()):\\\\n    try: eval(raw_input());\" \"\"\"\n",
        "\n",
        "# Encode user input\n",
        "user_input_embedding = encode_text(user_input)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarities = compute_cosine_similarity(user_input_embedding, embeddings_list)\n",
        "\n",
        "# Find the index of the most similar sample\n",
        "most_similar_index = np.argmax(similarities)\n",
        "\n",
        "# Get the most similar solution\n",
        "most_similar_solution = df.loc[most_similar_index, 'solutions']\n",
        "\n",
        "\n",
        "print(f\"Most Similar Sample: {most_similar_solution}\" , f'index: {most_similar_index}')\n",
        "print(f\"Cosine Similarity: {similarities[most_similar_index]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "### System Prompt\n",
        "You are an intelligent programming Tester.\n",
        "\n",
        "### User Message\n",
        "Generate test cases for the given functions written in C such that every line of code will be excuted once at least to achieve coverging 100%\n",
        "Consider all possible combinations for the set of inputs\n",
        "\n",
        "Example:\n",
        "    Input sample:\n",
        "      // A function that calculates the factorial of a positive integer\n",
        "      int factorial(int n) {\n",
        "        // Check if n is valid\n",
        "        if (n < 0) {\n",
        "          return -1;\n",
        "        }\n",
        "        // Base case\n",
        "        if (n == 0 || n == 1) {\n",
        "          return 1;\n",
        "        }\n",
        "        // Recursive case\n",
        "        return n * factorial(n - 1);\n",
        "      }\n",
        "\n",
        "in the previous example the only input is intgre so the possible combinations is zero or positive integre or negative one\n",
        "so the following test cases cover all these combinations\n",
        "\n",
        "    Corresponding Output:\n",
        "      {\n",
        "        \"test_case 1\": [-1],\n",
        "        \"test_case 2\": [-5],\n",
        "        \"test_case 3\": [0],\n",
        "        \"test_case 4\": [1],\n",
        "        \"test_case 5\": [5]\n",
        "      }\n",
        "\n",
        "### Assistant\n",
        "// A function that reverses a string\n",
        "char* reverse_string(char* str) {\n",
        "  // Get the length of the string\n",
        "  int len = strlen(str);\n",
        "  // Allocate memory for the reversed string\n",
        "  char* rev = malloc(len + 1);\n",
        "  // Check if the memory allocation was successful\n",
        "  if (rev == NULL) {\n",
        "    return NULL;\n",
        "  }\n",
        "  // Copy the characters from the original string to the reversed string in reverse order\n",
        "  for (int i = 0; i < len; i++) {\n",
        "    rev[i] = str[len - i - 1];\n",
        "  }\n",
        "  // Add the null terminator to the reversed string\n",
        "  rev[len] = '\\0';\n",
        "  // Return the reversed string\n",
        "  return rev;\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
        "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n",
        "\n",
        "# Use the tokenizer to encode the prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
        "# Use the model to generate text\n",
        "outputs = model2.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n",
        "# Use the tokenizer to decode the text\n",
        "text = tokenizer2.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "# Print the generated text\n",
        "print(text)\n",
        "\n",
        "import json\n",
        "\n",
        "# hn7ot el output bta3 el model\n",
        "json_string = '''\n",
        "{\n",
        "  \"test_case 1\": [\"Hello\"],\n",
        "  \"test_case 2\": [\"\"],\n",
        "  \"test_case 3\": [\"  Hello World  \"],\n",
        "  \"test_case 4\": [\"!@#$%^&*()\"],\n",
        "  \"test_case 5\": [\"12345\"]\n",
        "}\n",
        "'''\n",
        "data = json.loads(json_string)\n",
        "\n",
        "with open('output_parsing.txt', 'w') as txt_file:\n",
        "    for key, values in data.items():\n",
        "        txt_file.write(' '.join(map(str, values)) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GCOV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5psomWKpVqIT"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import platform\n",
        "\n",
        "file_name = 'reverse_string.cpp'\n",
        "input_file_name = 'output_parsing.txt'\n",
        "\n",
        "# Compilation command\n",
        "compilation_command = f\"g++ -o {file_name.split('.')[0]} -fprofile-arcs -ftest-coverage {file_name}\"\n",
        "\n",
        "# Executable name\n",
        "executable_name = \"./\" +file_name.split('.')[0]\n",
        "\n",
        "# Executing command\n",
        "executing_command = f\"./{executable_name} < {input_file_name}\" if platform.system() != 'Windows' else f\"{executable_name} < {input_file_name}\"\n",
        "\n",
        "# Coverage command\n",
        "coverage_command = f\"gcov {executable_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1y5ACMhVqIT",
        "outputId": "20202009-1af3-4556-cab5-5f48a6ca42e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compilation successful\n",
            "--------------------------------------------------\n",
            "olleH\n",
            "\n",
            "--------------------------------------------------\n",
            "File 'reverse_string.cpp'\n",
            "Lines executed:93.75% of 16\n",
            "Creating 'reverse_string.cpp.gcov'\n",
            "\n",
            "File '/usr/include/c++/11/iostream'\n",
            "No executable lines\n",
            "Removing 'iostream.gcov'\n",
            "\n",
            "Lines executed:93.75% of 16\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    subprocess.run(compilation_command, shell=True, check=True)\n",
        "    print(\"Compilation successful\\n\" + '-'*50)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Compilation failed with error: {e}\")\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(executing_command, shell=True, check=True, capture_output=True, text=True)\n",
        "    print(result.stdout + \"\\n\" + '-'*50)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Execution failed with error: {e}\")\n",
        "    print(\"Error Output:\", e.stderr)\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(coverage_command, shell=True, check=True, capture_output=True, text=True)\n",
        "    print(result.stdout + '\\n')\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Coverage failed with error: {e}\")\n",
        "    print(\"Error Output:\", e.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CSXK-HiixTK",
        "outputId": "2e1fc60d-639e-4569-e196-6f1056a2d664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        -:    0:Source:reverse_string.cpp\n",
            "        -:    0:Graph:./reverse_string.gcno\n",
            "        -:    0:Data:./reverse_string.gcda\n",
            "        -:    0:Runs:1\n",
            "        -:    1:// Include the iostream header file\n",
            "        -:    2:#include <iostream>\n",
            "        -:    3:#include <cstring>\n",
            "        -:    4:using namespace std;\n",
            "        -:    5:\n",
            "        -:    6:// A function definition for a function that reverses a string\n",
            "        1:    7:char* reverse_string(char* str) {\n",
            "        -:    8:    // Get the length of the string\n",
            "        1:    9:    int len = strlen(str);\n",
            "        -:   10:    // Allocate memory for the reversed string\n",
            "        1:   11:    char* rev = (char*)malloc(len + 1);\n",
            "        -:   12:    // Check if the memory allocation was successful\n",
            "        1:   13:    if (rev == NULL) {\n",
            "    #####:   14:    return NULL;\n",
            "        -:   15:    }\n",
            "        -:   16:    // Copy the characters from the original string to the reversed string in reverse order\n",
            "        6:   17:    for (int i = 0; i < len; i++) {\n",
            "        5:   18:    rev[i] = str[len - i - 1];\n",
            "        -:   19:    }\n",
            "        -:   20:    // Add the null terminator to the reversed string\n",
            "        1:   21:    rev[len] = '\\0';\n",
            "        -:   22:    // Return the reversed string\n",
            "        1:   23:    return rev;\n",
            "        -:   24:}\n",
            "        -:   25:\n",
            "        1:   26:int main() {\n",
            "        1:   27:    const int maxInputLength = 100;  // Adjust the maximum input length as needed\n",
            "        -:   28:    char input[maxInputLength];\n",
            "        -:   29:\n",
            "        -:   30:    // Get input from the user\n",
            "        1:   31:    std::cin.getline(input, maxInputLength);\n",
            "        -:   32:\n",
            "        -:   33:    // Call the reverse_string function\n",
            "        1:   34:    char* reversed = reverse_string(input);\n",
            "        -:   35:\n",
            "        -:   36:    // Display the result\n",
            "        1:   37:    std::cout << reversed << std::endl;\n",
            "        -:   38:\n",
            "        -:   39:    // Free the allocated memory\n",
            "        1:   40:    free(reversed);\n",
            "        -:   41:\n",
            "        1:   42:    return 0;\n",
            "        -:   43:}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Specify the file path\n",
        "file_path = file_name + '.gcov'  # Replace with the actual path to your text file\n",
        "\n",
        "# Open the file in read mode ('r' for reading)\n",
        "with open(file_path, 'r') as file:  # Remove quotes around file_path\n",
        "    # Read the contents of the file\n",
        "    file_contents = file.read()\n",
        "\n",
        "# Display or process the file contents\n",
        "print(file_contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV2Bg5D_i0Uv",
        "outputId": "5eaa5e08-7e70-42a7-881f-5c37eac6e5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Line 14: return NULL;\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Use regular expression to find lines after \"#####\"\n",
        "pattern = re.compile(r'#####:\\s+(\\d+):(.*?)\\n')\n",
        "matches = pattern.findall(file_contents)\n",
        "# Print the result\n",
        "# Print the result\n",
        "for match in matches:\n",
        "    line_number = match[0]\n",
        "    content = match[1].strip()\n",
        "    print(f\"Line {line_number}: {content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPRXdtGGVqIU",
        "outputId": "25662aea-c04a-4567-93ef-b39cc49895ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!g++ --version"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
