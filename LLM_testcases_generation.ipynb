{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1VpOAmF_mHR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers -U -q accelerate -U protobuf scipy utils datasets pandas matplotlib seaborn gdown sentencepiece nltk evaluate rouge_score\n",
    "!pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U einops\n",
    "!pip install -q -U safetensors\n",
    "!pip install -q -U torch\n",
    "!pip install -q -U xformers\n",
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "\n",
    "# # initialize the model\n",
    "# model_path = \"Phind/Phind-CodeLlama-34B-v2\"\n",
    "# model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "IsVRYZk310BD",
    "outputId": "5a4a9ccd-32d4-439d-8283-58a5c1dc3fc5"
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "### System Prompt\n",
    "You are an intelligent programming Tester.\n",
    "\n",
    "### User Message\n",
    "Generate test cases for the given functions written in C such that every line of code will be excuted once at least to achieve coverging 100%\n",
    "\n",
    "Example:\n",
    "    Input sample:\n",
    "      // A function that calculates the factorial of a positive integer\n",
    "      int factorial(int n) {\n",
    "        // Check if n is valid\n",
    "        if (n < 0) {\n",
    "          return -1;\n",
    "        }\n",
    "        // Base case\n",
    "        if (n == 0 || n == 1) {\n",
    "          return 1;\n",
    "        }\n",
    "        // Recursive case\n",
    "        return n * factorial(n - 1);\n",
    "      }\n",
    "\n",
    "    Corresponding Output:\n",
    "      {\n",
    "        \"test_case 1\": [-1],\n",
    "        \"test_case 2\": [-5],\n",
    "        \"test_case 3\": [0],\n",
    "        \"test_case 4\": [1],\n",
    "        \"test_case 5\": [5]\n",
    "      }\n",
    "\n",
    "### Assistant\n",
    "// A function that reverses a string\n",
    "char* reverse_string(char* str) {\n",
    "  // Get the length of the string\n",
    "  int len = strlen(str);\n",
    "  // Allocate memory for the reversed string\n",
    "  char* rev = malloc(len + 1);\n",
    "  // Check if the memory allocation was successful\n",
    "  if (rev == NULL) {\n",
    "    return NULL;\n",
    "  }\n",
    "  // Copy the characters from the original string to the reversed string in reverse order\n",
    "  for (int i = 0; i < len; i++) {\n",
    "    rev[i] = str[len - i - 1];\n",
    "  }\n",
    "  // Add the null terminator to the reversed string\n",
    "  rev[len] = '\\0';\n",
    "  // Return the reversed string\n",
    "  return rev;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqFXNRKf-dr-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_one_completion(prompt: str):\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "\n",
    "    # Generate\n",
    "    generate_ids = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n",
    "    completion = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    completion = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\n",
    "\n",
    "    return completion\n",
    "test_cases = generate_one_completion(prompt)\n",
    "print(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the tokenizer and the model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True).cuda()\n",
    "\n",
    "# Define a prompt that instructs the model to generate test cases for the factorial function\n",
    "prompt2 = \"\"\"\n",
    "Write test cases for the following function written in C:\n",
    "\n",
    "// A function that calculates the factorial of a positive integer\n",
    "int factorial(int n) {\n",
    "  // Check if n is valid\n",
    "  if (n < 0) {\n",
    "    return -1;\n",
    "  }\n",
    "  // Base case\n",
    "  if (n == 0 || n == 1) {\n",
    "    return 1;\n",
    "  }\n",
    "  // Recursive case\n",
    "  return n * factorial(n - 1);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True).cuda()\n",
    "# messages = [\n",
    "#     { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "# # 32021 is the id of <|EOT|> token\n",
    "# outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=32021)\n",
    "# print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tokenizer to encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "\n",
    "# Use the model to generate text\n",
    "outputs = model2.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n",
    "\n",
    "# Use the tokenizer to decode the text\n",
    "text = tokenizer2.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# Print the generated text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "\n",
    "file_name = 'sample.cpp'\n",
    "input_file_name = 'input.txt'\n",
    "\n",
    "# Compilation command\n",
    "compilation_command = f\"g++ -o {file_name.split('.')[0]} -fprofile-arcs -ftest-coverage {file_name}\"\n",
    "\n",
    "# Executable name\n",
    "executable_name = \"./\" +file_name.split('.')[0]\n",
    "\n",
    "# Executing command\n",
    "executing_command = f\"./{executable_name} < {input_file_name}\" if platform.system() != 'Windows' else f\"{executable_name} < {input_file_name}\"\n",
    "\n",
    "# Coverage command\n",
    "coverage_command = f\"gcov {executable_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    subprocess.run(compilation_command, shell=True, check=True)\n",
    "    print(\"Compilation successful\\n\" + '-'*50)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Compilation failed with error: {e}\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(executing_command, shell=True, check=True, capture_output=True, text=True)\n",
    "    print(result.stdout + \"\\n\" + '-'*50)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Execution failed with error: {e}\")\n",
    "    print(\"Error Output:\", e.stderr)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(coverage_command, shell=True, check=True, capture_output=True, text=True)\n",
    "    print(result.stdout + '\\n')\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Coverage failed with error: {e}\")\n",
    "    print(\"Error Output:\", e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
