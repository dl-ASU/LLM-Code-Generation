# LLM-Code-Generation

## To DO:

- Zero-Shot and Few-Shot -> everyone
- Retrieval Augmented Generation (RAG) -> Sara
- Chain-of-Thought Prompting -> Bayoumi (you should prepare a good documentation for promising models that we can use)
- Tree of Thoughts (ToT) -> Dallash
- Generated Knowledge Prompting -> Rana
- Self-Consistency -> Sameh (you should also prepare the parcing code and you can edit the output a little bit)

### Every one must prepare a prompt based on his/her corresponding Technique for our proplem and must provide to the model good knowledge and content from examples and different methods.

NOTE: If you done you can move through other techniques
