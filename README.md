# two papers summarization
## Predicting Code Coverage without Execution
#### In this paper, they created a new dataset called coverageEval, which is developed from HumanEval dataset. CoverageEval have many testcases to the same code block, and the code covered with each test case. Then, they use this dataset to test the ability of GPT-4, GPT-3, Bard and Anthropicâ€™s Claude. The results demonstrated that GPT-4 achieved the highest performance, with 10.46% exact match with zero-shot prompting and 24.48% with multi-shot prompting. However, none of the models, including GPT-4, achieved high accuracy in predicting code coverage, indicating that LLMs still have a long way to go in developing a deep understanding of code execution.

## Is Your Code Generated by ChatGPT Really Correct? (This paper is off topic, no need to look at)







# LLM-Code-Generation

#### The task is to search for open source LLMs that make good performance in the code coverage task then try to test it on colab.The LLMs that I faced was Starcoder, CodeGen , MOSA , Codex ( found out it is not open source ) ,beside list of other models that was not said directly to be used on code coverage problems but used generally on code generation tasks like codeLlama for instance. All those model has no interface on hugging face and could not either run on colab or run on local as they are too large to run on RAM. The next step is to buy some server and run try using models on it. I will leave some links of the targeted models to reach it easily.
##### CodeGen:
##### https://huggingface.co/Salesforce/codegen25-7b-multi 
##### https://github.com/salesforce/CodeGen
##### Starcoder:
##### https://huggingface.co/bigcode/starcoderbase-1b?text=def+print_hello_world%28%29%3A
##### https://github.com/bigcode-project/starcoder/tree/main
##### CodeBooga
#### https://huggingface.co/oobabooga/CodeBooga-34B-v0.1
